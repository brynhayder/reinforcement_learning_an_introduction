\input{header}

\begin{document}

{\huge Notes} \hfill {\huge Reinforcement Learning: An Introduction}\\
\Rule\\
\tableofcontents
\mbox{}\\
\Rule
\mbox{}\\


\clearpage
\section{Introduction}
Reinforcement learning is about how an agent can learn to interact with its environment. Reinforcement learning uses the formal framework of Markov decision processes to define the interaction between a learning agent and its environment in terms of states, actions, and rewards.

\setcounter{subsection}{2}
\subsection{Elements of Reinforcement Learning}
\begin{description}
     \item[Policy] defines the way that an agent acts, it is a mapping from perceived states of the world to actions. It may be stochastic.
     \item[Reward] defines the goal of the problem. A number given to the agent as a (possibly stochastic) function of the state of the environment and the action taken.
     \item[Value function] specifies what is good in the long run, essentially to maximise the expected reward. The central role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades.
     \item[Model] mimics the environment to facilitate planning. Not all reinforcement learning algorithms have a model (if they don't then they can't plan, i.e. must use trial and error, and are called model free).
\end{description}

\clearpage
\section{Multi-armed Bandits}
Reinforcement learning involves evaluative feedback rather than instructive feedback. We get told whether our actions are good ones or not, rather than what the single best action to take is. This is a key distinction between reinforcement learning and supervised learning.

\subsection{A $k$-armed Bandit Problem}
In the $k$-armed bandit problem there are $k$ possible actions, each of which yields a numerical reward drawn from a stationary probability distribution for that action. We want to maximise the expected total reward, taking an action at each \emph{time step}. Some notation:

\begin{itemize}
    \item Index timesteps by $t$
    \item Action $A_t$
    \item Corresponding reward $R_t$
    \item \emph{Value} of action $a$ is $q_*(a) = \mathbb{E}[R_t | A_t = a]$
    \item Estimate of value of action $a$ at $t$ is denoted $Q_t(a)$
\end{itemize}

We therefore want to choose $\{a_1, \dots, a_T\}$ to maximise $\sum_{t = 1}^T q_*(a_t)$.\\
\mbox{}\\ 
At each timestep, the actions with the highest estimated reward are called the \emph{greedy} actions. If we take this action, we say that we are \emph{exploiting} our understanding of the values of actions. The other actions are known as \emph{non-greedy} actions, sometimes we might want to take one of these to improve our estimate of their value. This is called \emph{exploration}. The balance between exploration and exploitation is a key concept in reinforcement learning.


\subsection{Action-value Methods}
We may like to form estimates of the values of possible actions and then choose actions according to these estimates. Methods such as this are known as \emph{action-value methods}. There are, of course, many ways of generating the estimates $Q_t(a)$. \\
\mbox{}\\
An $\varepsilon$-greedy method is one in which with probability $\varepsilon$ we take a random draw from all of the actions (choosing each action with equal probability), providing some exploration.


\setcounter{subsection}{4}
\subsection{Tracking a Non-stationary Problem}
If we decide to implement the sample average method, then at each iteration that we choose the given action we update our estimate by
\begin{equation}
    Q_{n+1} = Q_n + \frac1n [R_n - Q_n]
\end{equation}
Note that this has the (soon to be familiar) form
\begin{equation}
    \mathrm{NewEstimate} \gets \mathrm{OldEstimate} + \mathrm{StepSize}\times[\mathrm{Target} - \mathrm{OldEstimate}].
\end{equation}
\mbox{}\\
If the problem was non-stationary, we might like to use a time weighted exponential average for our estimates (\emph{exponential recency-weighted average}). This corresponds to a constant step-size $\alpha \in (0, 1]$ (you can check).
\begin{equation}
    Q_{n+1} = Q_n + \alpha [R_n - Q_n].
\end{equation}
\mbox{}\\
We might like to vary the step-size parameter. Write $\alpha_n(a)$ for the step-size after the $n^{\mathsf{th}}$ reward from action $a$. Of course, not all choices of $\alpha_n(a)$ will give convergent estimates of the values of $a$. To converge with probability 1 we must have
\begin{equation}
    \sum_n \alpha_n(a) = \infty \quad\quad \mathsf{and} \quad\quad  \sum_n \alpha_n(a)^2 < \infty.
\end{equation}
Meaning that the coefficients must be large enough to recover from initial fluctuations, but not so large that they don't converge in the long run. Although these conditions are used in theoretical work, they are seldom used in empirical work or applications. (Most reinforcement learning problems have non-stationary rewards, in which case convergence is undesirable.)

\subsection{Optimistic Initial Values}
The exponential recency weighted method is biased by the initial value one gives. If we like, we may set initial value estimates artificially high to encourage exploration in the short run -- this is called \emph{optimistic initial values}. This is a useful trick for stationary problems, but does not apply so well to non-stationary problems as the added exploration is only temporary.


\subsection{Upper-Confidence Bound Action Selection}
We might like to discriminate between potential explorative actions. Note that $\varepsilon$-greedy does not do this. We define the \emph{upper-confidence bound} action at $t$ as follows
\begin{equation}
    A_t \doteq \argmax_{a}\left[ \, Q_t(a)+ c \sqrt{\frac{\mathrm{ln}(t)}{N_t(a)}} \, \right]
\end{equation}
where $Q_t(a)$ is the value estimate for the action $a$ at time $t$, $c > 0$ is a parameter that controls the degree of exploration and $N_t(a)$ is the number of times that $a$ has been selected by time $t$. If $N_t(a) = 0$ then we consider $a$ a maximal action.\\

This approach favours actions with a higher estimated rewards but also favours actions with uncertain estimates (more precisely, actions that have been chosen few times).


\subsection{Gradient Bandit Algorithms}
Suppose that we choose actions probabilistically based on a preference for each action, $H_t(a)$. Let the action at $t$ be denoted by $A_t$. We then define the probability of choosing action $a$ via the softmax
\begin{equation}
    \pi_t(a) \doteq \P{}(A_t = a) = \frac{e^{H_t(a)}}{\sum_i e^{H_t(i)}}.
\end{equation}
We then iteratively perform updates according to 
\begin{equation}
    H_{t+1}(a) = H_t(a) + (R_t - \bar{R}_t)(\mathds{1}_{A_t = a} - \pi_t(a)),
\end{equation}
where $\bar{R}_t$ is the mean of previous rewards. The box in the notes shows that this is an instance of stochastic gradient ascent since the expected value of the update is equal to the update when doing gradient ascent on the (total) expected reward.

\clearpage
\section{Finite Markov Decision Processes}
We say that a system has the \emph{Markov property} if each state includes all information about the previous states and actions that makes a difference to the future.\\

The MDP provides an abstraction of the problem of goal-directed learning from interaction by modelling the whole thing as three signals: action, state, reward.\\

Together, the MDP and agent give rise to the \emph{trajectory} $S_0$, $A_0$, $R_1$, $S_1$, $A_1$, $S_2$, $R_2$, $\dots$. The action choice in a state gives rise (stochastically) to a state and corresponding reward.

\subsection{The Agent–Environment Interface}
We consider finite Markov Decision Processes (MDPs). The word finite refers to the fact that the states, rewards and actions form a finite set. This framework is useful for many reinforcement learning problems.\\

We call the learner or decision making component of a system the \emph{agent}. Everything else is the \emph{environment}. General rule is that anything that the agent does not have absolute control over forms part of the environment. For a robot the environment would include it's physical machinery. The boundary is the limit of absolute control of the agent, not of its knowledge.\\

The MDP formulation is as follows. Index time-steps by $t \in \mathbb{N}$. Then actions, rewards, states at $t$ represented by $A_t \in \mathcal{A}(s)$, $R_t \in \mathcal{R} \subset \mathbb{R}$, $S_t \in \mathcal{S}$. Note that the set of available actions is dependent on the current state.\\

A key quantity in an MDP is the following function, which defines the \emph{dynamics} of the system.
\begin{equation}
    p(s', r | s, a) \doteq \P{} (S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a)
\end{equation}
From this quantity we can get other useful functions. In particular we have the following: 

\begin{description}
    \item[state-transition probabilities]
\begin{equation}
    p(s' | s, a) \doteq \P{}(S_t = s'| S_{t-1} = s, A_{t-1}=A) = \sum_{r \in \mathcal{R}} p(s', r | s, a)
\end{equation}
note the abuse of notation using $p$ again; and,
    \item[expected reward]
\begin{equation}
    r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a).
\end{equation}
\end{description}


\subsection{Goals and rewards}
We have the \emph{reward hypothesis}, which is a central assumption in reinforcement learning:
\begin{quote}
    All of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal (called reward).
\end{quote}


\subsection{Returns and Episodes}
Denote the sequence of rewards from time $t$ as $R_{t+1}$, $R_{t+2}$, $R_{t+3}$, $\dots$. We seek to maximise the \emph{expected return} $G_t$ which is some function of the rewards. The simplest case is where $G_t = \sum_{\tau > t} R_\tau$.\\

In some applications there is a natural final time-step which we denote $T$. The final time-step corresponds to a \emph{terminal state} that breaks the agent-environment interaction into subsequences called \emph{episodes}. Each episode ends in the same terminal state, possibly with a different reward. Each starts independently of the last, with some distribution of starting states. We denote the set of states including the terminal state as $\mathcal{S}^+$\\

Sequences of interaction without a terminal state are called \emph{continuing tasks}. \\

We define $G_t$ using the notion of \emph{discounting}, incorporating the \emph{discount rate} $0 \leq \gamma \leq 1$. In this approach the agent chooses $A_t$ to maximise 
\begin{equation}
    G_t \doteq \sum_{k = 0}^{\infty} \gamma^k R_{t+k+1}.
\end{equation}
 This sum converges wherever the sequence $R_t$ is bounded. If $\gamma = 0$ the agent is said to be myopic. We define $G_T = 0$. Note that
\begin{equation}
     G_t = R_{t+1} + \gamma G_{t+1}.
\end{equation}\\

Note that in the case of finite time steps or an episodic problem, then the return for each episode is just the sum (or whatever function) of the returns in that episode.


\subsection{Unified Notation for Episodic and Continuing Tasks}
We want to unify the notation for episodic and continuing learning. \\

We introduce the concept of an \emph{absorbing state}. This state transitions only to itself and gives reward of zero.\\

To incorporate the (disjoint) possibilites that $T=\infty$ or $\gamma = 1$ in our formulation of the return, we might like to write
\begin{equation}
    G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1}R_k.
\end{equation}


\subsection{Policies \& Value Functions}
\subsubsection*{Policy}
A \emph{policy} $\pi(a|s)$ is a mapping from states to the probability of selecting actions in that state. If an agent is following policy $\pi$ and at time $t$ is in state $S_t$, then the probability of taking action $A_t$ is $\pi(a|s)$. Reinforcement learning is about altering the policy from experience.\\

\subsubsection*{Value Functions}
As we have seen, a central notion is the value of a state. The \emph{state-value function} of state $s$ under policy $\pi$ is the expected return starting in $s$ and following $\pi$ thereafter. For MDPs this is
\begin{equation}
    v_\pi \doteq \Epi[G_t | S_t = s],
\end{equation}
where the subscript $\pi$ denotes that this is an expectation taken conditional on the agent following policy $\pi$. \\

Similarly, we define the \emph{action-value function} for policy $\pi$ to be the expected return from taking action $a$ in state $s$ and following $\pi$ thereafter
\begin{equation}
    q_\pi(s, a) \doteq \Epi[G_t | S_t = s, A_t = a].
\end{equation}

The value functions $v_\pi$ and $q_\pi$ can be estimated from experience.\\

\subsubsection*{Bellman Equation}

The Bellman equations express the value of a state in terms of the value of its successor states. They are a consistency condition on the value of states. 

\begin{align}
    v_{\pi}(s) &= \Epi{}[G_t | S_t = s] \\
             &= \Epi{}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
             &= \sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[r + \gamma \Epi{}[G_{t+1} | S_{t+1} = s']\right] \\
             &=  \sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')]
\end{align} 
    

The value function $v_\pi$ is the unique solution to its Bellman equation.


\subsection{Optimal Policies \& Optimal Value Functions}
We say that $\pi \geq \pi'$ iff $v_\pi (s) \geq v_{\pi'}(s) \quad \forall s \in \mathcal{S}$. The policies that are optimal in this sense are called optimal policies. There may be multiple optimal policies. We denote all of them by $\pi_*$.\\

The optimal policies share the same optimal value function $v_*(s)$
\begin{equation}
    v_*(s) \doteq \max_\pi v_\pi(s) \quad \forall s \in \mathcal{S}.
\end{equation}
They also share the same optimal action-value function $q_*(s, a)$
\begin{equation}
    q_*(s, a) = \max_\pi q_\pi (s, a) \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s),
\end{equation}
this is the expected return from taking action $a$ in state $s$ and thereafter following the optimal policy.
\begin{equation}
    q_*(s, a) = \E{} [R_{t+1} + \gamma v_*(S_{t+1}) | S_{t} = s, A_t = a].
\end{equation}\\

Since $v_*$ is a value function, it must satisfy a Bellman equation (since it is simply a consistency condition). However, $v_*$ corresponds to a policy that always selects the maximal action. Hence 
\begin{equation}
    v_*(s) = \max_a \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')].
\end{equation}
Similarly,
\begin{align}
    q_*(s, a) &= \mathbb{E} [R_{t+1} + \gamma \max_{a'}q_*(S_{t+1}, a') | S_t=s, A_t = a]\\
              &= \sum_{s', r} p(s', r| s, a ) [r + \gamma \max_{a'}q_*(s', a')].
\end{align} \\

Note that once one identifies an optimal value function $v_*$, then it is simple to find an optimal policy. All that is needed is for the policy to act greedily with respect to $v_*$. Since $v_*$ encodes all information on future rewards, we can act greedily and still make the long term optimal decision (according to our definition of returns).\\

Having $q_*$ is even better since we don't need to check $v_*(s')$ in the succeeding states $s'$, we just find $a_* = \argmax_a q_*(s, a)$ when in state $s$.

\clearpage
\section{Dynamic Programming}

The term Dynamic Programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given perfect model of the environment as a Markov Decision Process (MDP). DP methods tend to be computationally expensive and we often don't have a perfect model of the environment, so they aren't used in practice. However, they provide useful theoretical basis for the rest of reinforcement learning. \\

Unless stated otherwise, will assume that the environment is a finite MDP. If the state or action space is continuous, then we will generally discretise it and apply finite MDP methods to the approximated problem.\\

The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. We use DP and the Bellman equations to find optimal value functions. 

\subsection{Policy Evaluation (Prediction)}
We can use the Bellman equation for the state-value function $v_\pi$ to construct an iterative updating procedure.

\subsubsection*{Iterative Policy Evaluation}
Consider a sequence of approximate value functions $v_0, v_1, v_2, \dots$ each mapping $\mathcal{S}^{+}$ to $\mathbb{R}$. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value $0$), and each successive approximation is obtained by using the Bellman equation for $v_\pi$ as an update rule:

\begin{align}
    v_{k+1} &\doteq \Epi [R_{t+1} + \gamma v_{k}(S_{t+1}) | S_t = s] \\
            &= \sum_a \pi(s|a) \sum_{s', r} p(s', r| s, a) \left[r + \gamma v_k(s')\right]
\end{align}

Clearly, $v_k = v_\pi$ is a fixed point. The sequence $\{v_k\}$ can be shown in general to converge to $v_\pi$ as $k \to \infty$ under the same conditions that guarantee the existence of $v_\pi$. This algorithm is called \emph{iterative policy evaluation}. This update rule is an instance of an \emph{expected update} because it performs the updates by taking an expectation over all possible next states rather than by taking a sample next state.\\

\subsection{Policy Improvement}
\subsubsection*{Policy Improvement Theorem}

Let $\pi$, $\pi'$ be any pair of deterministic policies, such that
\begin{equation}
    q_\pi(s, \pi(s)) \geq v_\pi(s) \quad \forall s \in \mathcal{S}.
\end{equation}
That is, $\pi'$ is as least as good as $\pi$. Then we have (shown below)
\begin{equation}
    v_{\pi'}(s) \geq v_\pi(s) \quad \forall s \in \mathcal{S}
\end{equation}
so $\pi'$ gives at least as good (expected) return as $\pi$.\\

The argument below also shows that if $q_\pi(s, \pi(s)) > v_\pi(s)$ at any $s$, then there is at least one $s$ for which $v_{\pi'}(s) > v_\pi(s)$.
\subsubsection*{proof:}
\begin{align*}
    v_\pi(s) & \leq q_\pi(s, \pi'(s)) \\
             & = \E{}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=\pi'(s)] \\
             & = \E{}_{\pi'} [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s] \\
             & \leq \E{}_{\pi'} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots| S_t=s] \\
             & = v_{\pi'}(s)
\end{align*}

\subsubsection*{Policy Improvement Algorithm}
Now consider a policy that is greedy with respect to $q_\pi(s, a)$. Define 
\begin{align}
    \pi'(s) &= \argmax_a q_\pi(s, a) \\ 
            &= \argmax_a \E{} [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=a] \\
            & \argmax_a \sum_{s', r} p(s', r|s, a)[ r + \gamma v_\pi(s')].
\end{align}
Now we can use $v_\pi$ to get $\pi' \geq \pi$, then use $v_{\pi'}$ to get \emph{another} policy. (In the above, ties are broken arbitrarily when the policy is deterministic. If the policy is stochastic, we accept any policy that assigns zero probability to sub-optimal actions.)\\

Note that by construction
\[
    q_\pi(s, \pi'(s)) \geq v_\pi(s)
\]
therefore
\[
    v_{\pi'} \geq v_\pi
\]
so we get from this process a monotonically increasing sequence of policies.\\

Note also that is $\pi'$ is as good as $\pi$ then $v_{\pi'} = v_\pi$ and $\forall s \in \mathcal{S}$
\begin{align*}
    v_\pi &= \max_a \E{}[R_{t+1} + \gamma v_{\pi'(S_{t+1})}| S_t=s, A_t=a]\\
          &= \max_a \sum_{s', r} p(s', r|s, a)(r + \gamma v_{\pi'}(s'))
\end{align*}
which is the Bellman optimality condition for $v_*$, so both $\pi$ and $\pi'$ are optimal. This means that policy improvement gives a strictly better policy unless the policy is already optimal. \\

The policy improvement theorem holds for stochastic policies too, but we don't go into that here.
          

\subsection{Policy Iteration}
We can exploit policy improvement iteratively to get the policy iteration algorithm.

\includegraphics[width=\textwidth]{data/notes_images/policy_iteration_algorithm.png}
\mbox{}\\
A finite MDP has only a finite number of policies (as long as they are deterministic, of course) so this process is guaranteed to converge.

\subsection{Value Iteration}
Policy iteration can be slow because each iteration involves running the entire policy evaluation until convergence. \\

It turns out that one can truncate the policy evaluation step of policy iteration in many ways without losing convergence guarantees. One special case of this is \emph{value iteration}, where we truncate policy evaluation after only one update of each state. This algorithm converges to $v_*$ under the same conditions that guarantee the existence of $v_*$. 

\includegraphics[width=\textwidth]{data/notes_images/value_iteration_algorithm.png}
\mbox{}\\

Note the $\max_a$ in the assignment of $V(s)$, since we only one sweep of the state space and then choose the greedy policy.\\

It may be more efficient to interpose multiple policy evaluation steps in between policy improvement iterations, all of these algorithms converge to an optimal policy for discounted finite MDPs. 

\subsection{Asynchronous Dynamic Programming}
The DP methods that we have described so far all involve a full sweep of the state space on each iteration. This is potentially a very costly procedure. \\

\emph{Asynchronous} DP algorithms update the values in-place and cover states in any order whatsoever. The values of some states may be updated several times before the values of others are updated once. To converge correctly, however, an asynchronous algorithm must continue to update the values of all the states: it can’t ignore any state after some point in the computation.\\

Asynchronous DPs give a great increase in flexibility, meaning that we can choose the updates we want to make (even stochastically) based on the interaction of the agent with the environment. This procedure might not reduce computation time in total if the algorithm is run to convergence, but it could allow for a better rate of progress for the agent.

\subsection{Generalised Policy Iteration}
We use the term \emph{generalised policy iteration} (GPI) to refer to the general idea of letting policy evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI, including the policy iteration algorithms we have discussed in this section. GPI works via the competing but complementary nature of the two processes. In some cases it can be guaranteed to converge. 

\subsection{Efficiency of Dynamic Programming}
If we ignore a few technical details, then the (worst case) time DP methods take to find an optimal policy is polynomial in the number of states and actions. Compare this to the searching the states directly, which is exponential.


\clearpage
\section{Monte Carlo Methods}
Monte Carlo methods learn state and action values by sampling and averaging returns (i.e. not from dynamics like DP). These methods learn from experience (real or simulated) and require no prior knowledge of the environments dynamics.\\

Monte Carlo methods tus require well defined returns, so we will consider them only for episodic tasks. Only on completion of an episode do values and policies change.\\

We still use the generalised policy iteration framework, but we adapt it so that we learn the value function from experience rather than compute it \emph{a priori}.


\subsection{Monte Carlo Prediction}
The idea is to average the returns following each state to get an estimate of the state value
\[
    v_\pi(s) = \Epi{} [G_{t+1} | S_t =s].
\]
Given enough observations, the sample average converges to the true state value under the policy $\pi$.\\

Given a policy $\pi$ and a set of episodes, here are two ways in which we might estimate state values
\begin{description}
    \item[First Visit MC] average returns from first visit to state $s$ in order to estimate $v_\pi(s)$
    \item[Every Visit MC] average returns following every visit to state $s$.
\end{description}

First visit MC generates iid estimates of $v_\pi(s)$ with finite variance, so the sequence of estimates converges to the expected value by the law of large numbers as visits to $s$ tend to $\infty$. Every visit MC does not generate independent estimates, but still converges.\\

An algorithm for first visit MS (what we will focus on) is below. Every visit is the same, just without the check for $S_k$ occurring earlier in the episode.

\includegraphics[width=\textwidth]{data/notes_images/first_visit_mc_algo.png}

Monte Carlo methods are often used even when the dynamics of the environment are knowable, e.g. in Blackjack. It is often much easier to create sample games than it is to calculate environment dynamics directly.\\

MC estimates for different states are independent (unlike bootstrapping in DP). This means that we can use MC to calculate the value function for a subset of the states, rather than the whole state space as with DP. Along with the ability to learn from experience and simulation, this is the another advantage that MC has over DP.

\subsection{Monte Carlo Estimation of Action Values}
If we don't have a model for the environment, then it is more useful to estimate action-values. With a model we can use state values to find a policy by searching possible actions, as with DP (value iteration, etc.). We can't do this without knowledge of the dynamics, so one of the primary goals of MC is to estimate $q_*$. We start with policy evaluation for action-values.

\subsubsection*{Policy Evaluation for Action-Values}
The policy evaluation problem for action-values is to estimate $q_\pi(s, a)$ for some $\pi$. This is essentially the same as for state values, only we now talk about state-action pairs being visited, i.e. taking action $a$ in state $s$, rather than just states being visited.\\

If $\pi$ is deterministic, then we will only estimate the values of actions that $\pi$ dictates. We therefore need to incorporate some exploration in order to have useful action-values (since, after all, we want to use them to make informed decisions).\\

One consideration is to make $\pi$ stochastic, e.g. $\varepsilon$-soft. Another is the assumption of \emph{exploring starts}, which specifies that ever state-action pair has non-zero probability of being selected as the starting state. Of course, this is not always posible in practice.\\

For now we assume exploring start. Later we will come back to the issue of \emph{maintaining exploration}

\subsection{Monte Carlo Control}
We make use of the GPI framework for action-values. Policy evaluation is done as described. Policy improvement is done by making the policy greedy with respect to the action-value function, so no model is needed for this step
\[
    \pi(s) \doteq \argmax_a q_(s, a).
\]

We generate a sequence of policies $\pi_k$ each greedy with respect to $q_{\pi_{k-1}}(s, a)$. The policy improvement theorem applies: for all $s \in \mathcal{S}$
\begin{align}
    q_{\pi_k}(s, a) &= q_{\pi_k}(s, \argmax_a q_\pi(s, a)) \\
                    &= \max_a q_{\pi_k}(s, a) \\
                    \geq q_{\pi_k}(s, \pi_k(s))\\
                    = v_{\pi_k}(s)
\end{align}

So $\pi_{k+1}$ uniformly better than $\pi_k$ or it is optimal. \\

The above procedure's convergence depends on assumptions of exploring starts and infinitely many episodes. We will relax the first later, but we will address the second now.\\

Two approaches to avoid infinitely many episodes:
\begin{enumerate}
    \item Stop the algorithm once the $q_{\pi_k}$ stop moving within a certain error. (In practice this is only useful on the smallest problems.)
    \item Stop policy evaluation after a certain number of episodes, moving the action value towards $q_{\pi_k}$, then go to policy improvement.
\end{enumerate}

For MC policy evaluation, it is natural to alternate policy evaluation and improvement on a episode by episode basis. We give such an algorithm below (with the assumption of exploring starts).

\includegraphics[width=\textwidth]{data/notes_images/mc_policy_iteration_exploring_starts.png}

It is easy to see that optimal policies are a fixed point of this algorithm. Whether this algorithm converges in general is still, however, an open question.

\subsection{Monte Carlo Control without Exploring Starts}
\subsubsection*{On Policy vs. Off Policy}
On-policy methods evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve one that is different than the one used to generate the data.

\subsubsection*{On-Policy Techniques without Exploring Starts}
We consider $\varepsilon$-greedy policies that put probability $1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}$ on the maximal action and $\frac{\varepsilon}{|\mathcal{A}(s)|}$ on each of the others. These are examples of $\varepsilon$-soft policies in which $\pi(a|s) \geq \frac{\varepsilon}{|\mathcal{A}(s)|}$.\\

We use this idea in the GPI framework:
\includegraphics[width=\textwidth]{data/notes_images/on_policy_mc_algo.png}

We now show that an $\varepsilon$-greedy policy with respect to $q_\pi$, $\pi'$, is an improvement over any $\varepsilon$-soft policy $\pi$. For any $s \in \mathcal{S}$
\begin{align}
    q_\pi(s, \pi'(s)) &= \sum_a \pi'(a|s) q_\pi(s, a) \\ 
                      &= \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_a q_\pi(s, a) + (1 - \varepsilon)\max_a q_\pi(s, a) \\
                      &\geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_a q_\pi(s, a) + (1 - \varepsilon)\sum_a\frac{\pi(a|s) - \frac{\varepsilon}{|\mathcal{A}(s)|}}{1 - \varepsilon} q_\pi(s, a) \\
                      &= \sum_a \pi(a|s) q_\pi(s, a)\\
                      &= v_\pi(s)
\end{align}
(where line 3 follows because a weighted average with weights $w_i \geq 0$ and $\sum_i w_i = 1$ is $\leq$ the max term).\\

This satisfies the condition of the policy improvement theorem so we now know that $\pi' \geq \pi$.\\

Previously, with deterministic greedy policies, we would get automatically that fixed points of policy iteration are optimal policies since
\[
    v_*(s) \doteq max_\pi v_\pi(s) \quad \forall s \in \mathcal{S}.
\]
Now our policies are not deterministically greedy, our value updates do not take this form. We note, however, that we can consider an equivalent problem where we change the environment to select state and reward transitions at random with probability $\varepsilon$ and do what our agent asks with probability $1 - \varepsilon$. We have moved the stochasticity of the policy into the environment, creating an equivalent problem. The optimal value function in the new problem satisfies its Bellman equation
\begin{align}
    \tilde{v}_\pi(s) &= (1 - \varepsilon) \max_a \tilde{q}_\pi(s, a) + \frac{\varepsilon}{|\mathcal{A}(s)|}\sum_a \tilde{q}_\pi(s, a) \\ 
                     &= (1 - \varepsilon) \max_a \sum_{s', r} p(s', r|s, a)[r + \gamma \tilde{v}_\pi(s')] + \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_a \sum_{s', r} p(s', r|s, a)[r + \gamma \tilde{v}_\pi(s')].
\end{align}
We also know that at fixed points of our algorithm
\begin{align}
    v_\pi(s) &= (1 - \varepsilon) \max_a q_\pi(s, a) + \frac{\varepsilon}{|\mathcal{A}(s)|}\sum_a q_\pi(s, a) \\ 
                     &= (1 - \varepsilon) \max_a \sum_{s', r} p(s', r|s, a)[r + \gamma v_\pi(s')] + \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_a \sum_{s', r} p(s', r|s, a)[r + \gamma v_\pi(s')].
\end{align}
This is the same equation as above, so by uniqueness of solutions to the Bellman equation we have that $v_\pi = \tilde{v}_\pi$ and so $\pi$ is optimal.

\subsection{Off-Policy Prediction via Importance Sampling}
Off-policy learning uses information gained by sampling the \emph{behaviour policy} $b$ to learn the \emph{target policy} $\pi$. The behaviour policy explores the environment for us during training and we update the target policy accordingly.\\

In this section we consider the prediction problem: estimating $v_\pi$ or $q_\pi$ for a fixed and known $\pi$ using returns from $b$. In order to do this we need the assumption of coverage:
\begin{equation}
    \pi(a|s) \geq 0 \implies b(a|s) \geq 0.
\end{equation}
This implies that $b$ must be stochastic wherever it is not identical to $\pi$. The target policy $\pi$ may itself be deterministic, e.g. greedy with respect to action-value estimates.

\subsubsection*{Importance Sampling}
We use \emph{importance sampling} to evaluate expected returns from $\pi$ given returns from $b$.\\

Define the importance sampling ratio as the relative probability of a certain trajectory from $S_t$
\begin{align}
    \rho_{t:T-1} &= \frac{\P{}(A_t, S_{t+1}, A_{t+1}, \dots)| S_t, A_{t:T-1} \sim \pi}{\P{}(A_t, S_{t+1}, A_{t+1}, \dots)| S_t, A_{t:T-1} \sim b} \\
                 &= \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)\P{}(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)\P{}(S_{k+1}|S_k, A_k)}\\
                 &=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}
\end{align}
where the state transition dynamics $\P{}$ cancel out.\\

If we have returns $G_t$ from evaluating policy $b$, so $v_b(s) = \E[G_t|S_t=s]$, then we can calculate
\[
    v_\pi(s) = \E[\rho_{t:T-1}G_t|S_t=s]
\]

\subsubsection*{Estimation}
Introduce new notation:
\begin{itemize}
    \item Label all time steps in a single scheme. So maybe episode 1 is $t=1, \dots, 100$ and episode 2 is $t = 101, \dots, 200$, etc.
    \item Denote the set times of first/every visit to $s$ by $\mathcal{T}(s)$ (spanning episodes).
    \item Let $T(t)$ be the first termination after $t$
    \item Let $G_t$ be the returns from $t$ to $T(t)$
\end{itemize}

We can now give two methods of values for $\pi$ from returns from $b$:
{\bfseries{Ordinary Importance Sampling}}
\begin{equation}
    V(s) \doteq \frac{\sum_{t\in \mathcal{T}(s)} \rho_{t:T-1}G_t}{|\mathcal{T}(s)|}
\end{equation}
{\bfseries{Weighted Importance Sampling}}
\begin{equation}
    V(s) \doteq \frac{\sum_{t\in \mathcal{T}(s)} \rho_{t:T-1}G_t}{\sum_{t\in \mathcal{T}(s)} \rho_{t:T-1}}
\end{equation}
or 0 if the denominator is 0.\\

Weighted importance sampling is biased (e.g. it's expectation is $v_b(s)$ after 1 episode) but has bounded variance. The ordinary importance sampling ratio is unbiased, but has possibly infinite variance, because the variance of the importance sampling ratios themselves is unbounded.\\

Assuming bounded returns, the variance of the weighted importance sampling estimator converges to 0 even if the variance of the importance sampling ratios is infinite. In practice, this estimator usually has dramatically lower variance and is strongly preferred.

\subsection{Incremental Implementation}
We look for incremental calculations of the averages that make up the estimates, as in Chapter 2.\\

For on-policy methods the incremental averaging is the same as in Chapter 2. For off-policy methods, but with ordinary importance sampling, we only need to multiply the returns by the importance sampling ratio and then we can average as before.\\

We will now consider weighted importance sampling. We have a sequence of returns $G_i$, all starting in the same state $s$ and each with a random weight $W_i$ (e.g. $W_i = \rho_{i:T(i)-1}$). We want to iteratively calculate (for $n \geq 2$)
\[
    V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}.
\]
We can do this with the following update rules
\begin{align}
    V_{n+1} &= V_n + \frac{W_n}{C_n}[G_n - V_n]\\
    C_{n+1} &= C_n + W_{n+1}
\end{align}
where $C_0 = 0$ and $V_1$ is arbitrary (notice that it cancels out as $V_2 = G_1$).\\

Below is an algorithm for off-policy weighted importance sampling (set $b=\pi$ for on policy). The estimator $Q$ converges to $q_\pi$ for all encountered state-action pairs.

\includegraphics[width=\textwidth]{data/notes_images/off_policy_mc_prediction_algo.png}


\subsection{Off-Policy Monte Carlo Control}
Below is an algorithm for estimating $\pi_*$ and $q_*$ in the GPI framework. The target policy $\pi$ is the greedy policy with respect to $Q$, which is an estimate of $q_\pi$. This algorithm converges to $q_\pi$ as long as an infinite number of returns are observed for each state-action pair. This can be achieved by making $b$ $\varepsilon$-soft. The policy $\pi$ converges to $\pi_*$ at all encountered states even if $b$ changes (to another $\varepsilon$-soft policy) between or within episodes.

\includegraphics[width=\textwidth]{data/notes_images/off_policy_mc_control_algo.png}

Notice that this policy only learns from episodes in which $b$ selects only greedy actions after some timestep. This can greatly slow learning.

\subsection{*Discounting Aware Importance Sampling}
We present a method of importance sampling that recognises the return as a discounted sum of rewards. This can help in estimation, since if an episode is of length 100 and $\gamma = 0$ then the final 99 terms of the importance sampling ration contribute nothing to the expected value of our estimator (they have expected value of 1) but can greatly increase its variance. We therefore construct a method of importance sampling that takes into account discounting.\\

Introduce the \emph{flat partial returns}
\[
    \bar{G}_{t:h} \doteq \sum_{i=t+1}^h R_{i} \quad 0 \leq t \leq h \leq T
\]
then it can be shown (by rearranging) that
\begin{align}
    G_t &\doteq \gamma^{i-t}R_{i+1}\\
        &= (1 - \gamma)\sum_{h=t+1}^{T-1}\gamma^{h-t-1}\bar{G}_{t:h} + \gamma^{T-t-1}\bar{G}_{t:T}.
\end{align}
Now we can scale each flat partial return by a truncated importance sampling ratio (hence reducing variance).\\

{\bfseries{Ordinary Importance Sampling Ratio}}
\begin{equation}
    V(s) \doteq \frac{\sum_{t\in \mathcal{T}(s)} \left[ (1 - \gamma) \sum_{h=t+1}^{T(t-1)} \gamma^{h - t - 1}\rho_{t:h-1}\bar{G}_{t:h} + \gamma^{T(t) - t - 1}\rho_{t:T(t) - 1} \bar{G}_{t:T(t)} \right]}{|\mathcal{T}(s)|}
\end{equation}

{\bfseries{Weighted Importance Sampling Ratio}}
\begin{equation}
    V(s) \doteq \frac{\sum_{t\in \mathcal{T}(s)} \left[ (1 - \gamma) \sum_{h=t+1}^{T(t-1)} \gamma^{h - t - 1}\rho_{t:h-1}\bar{G}_{t:h} + \gamma^{T(t) - t - 1}\rho_{t:T(t) - 1} \bar{G}_{t:T(t)} \right]}{\sum_{t\in \mathcal{T}(s)} \left[ (1 - \gamma) \sum_{h=t+1}^{T(t-1)} \gamma^{h - t - 1}\rho_{t:h-1} + \gamma^{T(t) - t - 1}\rho_{t:T(t) - 1} \right]}
\end{equation}

\subsection{*Per-Decision Importance Sampling}
There is another way in which we may be able to reduce variance in off-policy importance sapling, even in the absence of discounting ($\gamma = 1$). Notice that the off-policy estimators are made up of terms like
\[
    \rho_{t:T-1}G_t = \rho_{t:T-1} (R_{t+1} + \gamma R_{t+2} + \dots+ \gamma^{T-t-1}R_{T})
\]
and that each of these terms is of the form
\[
    \rho_{t:T-1}R_{t+1} = \frac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\frac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}R_{t+1}.
\]
Now notice that only the first and last terms here are correlated, while all the others have expected value 1 (taken with respect to $b$). Clearly this is also the case at each $t$. This means that
\[
    \E{}[\rho_{t:T-1}R_{t+k}] = \E{}[\rho_{t:t+k-1}R_{t+k}]
\]
therefore
\[
    \E{}[\rho{t:T-1}G_t] = \E{}[\tilde{G}_t]
\]
where
\[
    \tilde{G}_t \doteq \sum_{i=t}^{T-1}\gamma^{i-t}\rho_{t:i}R_{i+1}.
\]
Now we can write the ordinary importance sampling estimator as
\[
    V(s) \doteq \frac{\sum_{t\in\mathcal{T}(s)} \tilde{G}_t}{|\mathcal{T}(s)|}
\]
possibly reducing variance in the estimator. \\

The weighted importance sampling estimators of this form that have so far been found have been shown to not be consistent (in the statistical sense). We don't know if a consistent weighted average form of this exists.

\clearpage
\section{Temporal-Difference Learning}
\end{document}